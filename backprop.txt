import numpy as np
X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)
y = np.array(([92], [86], [89]), dtype=float)
X = X / np.amax(X, axis=0)
y = y / 100
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def derivatives_sigmoid(x):
    return x * (1 - x)

epochs = 7000
learning_rate = 0.1
input_neurons = 2
hidden_neurons = 3
output_neurons = 1

wh = np.random.uniform(size=(input_neurons, hidden_neurons))
bh = np.random.uniform(size=(1, hidden_neurons))
wo = np.random.uniform(size=(hidden_neurons, output_neurons))
bo = np.random.uniform(size=(1, output_neurons))

for i in range(epochs):
    net_h = np.dot(X, wh) + bh
    sigma_h = sigmoid(net_h)
    net_o = np.dot(sigma_h, wo) + bo
    output = sigmoid(net_o)
    
    error = y - output
    mean_squared_error = np.mean(error**2)
    delta_output = error * derivatives_sigmoid(output)
    delta_hidden = delta_output.dot(wo.T) * derivatives_sigmoid(sigma_h)
    wo += sigma_h.T.dot(delta_output) * learning_rate
    wh += X.T.dot(delta_hidden) * learning_rate
    bo += np.sum(delta_output, axis=0, keepdims=True) * learning_rate
    bh += np.sum(delta_hidden, axis=0, keepdims=True) * learning_rate
    
    if i % 1000 == 0:  
        print('Epoch -> {0}, Learning rate -> {1}, Mean Squared Error -> {2}'.format(i, learning_rate, mean_squared_error))
        
print("Input: \n" + str(X))
print("Actual Output: \n" + str(y))
print("Predicted Output: \n" + str(output))
